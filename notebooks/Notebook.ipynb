{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e7e09b-63c6-4663-b079-68637d5cdd91",
   "metadata": {},
   "source": [
    "### **Let's explore Ray for an End-to-End Deep Learning Project!**\n",
    "\n",
    "Welcome to an exploration of Ray, an open-source framework that empowers us to tackle the formidable challenges of training large deep learning models. You might be wondering, why Ray? Over the past four years, my curiosity has been fueled by the desire to understand how these massive models are trained. Think about it&mdash;training a vision transformer on your laptop with millions of parameters or even conducting ablation studies seems like an insurmountable task.\n",
    "\n",
    "That's where Ray steps in. This remarkable framework offers distributed computing capabilities that enable us to train colossal models swiftly. It eliminates the need for expertise in infrastructure management, taking care of the heavy lifting. Moreover, transitioning from local development to a cloud environment is a breeze with Ray; no drastic code changes required. For an in-depth understanding of the framework, I urge you to refer to their [website](https://www.ray.io/). \n",
    "\n",
    "\n",
    "#### **Exploring this Repository**\n",
    "\n",
    "Before we delve into the nitty-gritty details of this implementation, I'd like to extend my heartfelt gratitude to [Goku Mohandas](https://www.linkedin.com/in/goku/) for his outstanding [Made-With-ML](https://github.com/GokuMohandas/Made-With-ML) repository&mdash;a phenomenal source for MLOps; Their dedication to advancing the field and sharing knowledge has been a guiding light for countless enthusiasts, including myself. \n",
    "\n",
    "Now, let's briefly discuss the different modules applied in this implementation. \n",
    "\n",
    "1. **utils.py**: This module covers two functions focused on reading the configuration file (i.e., config.YAML) and setting the tracking URI for mlflow. \n",
    "2. **data_utils.py**: This module prepares the image classification data. This should be changed as required. Note that, in this case, we are working with Image Data. It covers two scenarios: when the data is stored locally and when the data needs to be downloaded. For simplicity, we can use the data from an online source.\n",
    "3. **build_model.py**: This module defines the model for training. In this case, we are fine-tuning a ResNet50 Model. Therefore, it only initializes it with pre-trained weights. In this module, we can define custom Pytorch models as well.\n",
    "4. **train_utils.py**: This module has functions that will be used in the training process.\n",
    "5. **train_engine.py**: This is the module where we train the model using the Ray Framework. The critical concepts of Ray Train can be studied in this [documentation](https://docs.ray.io/en/latest/train/key-concepts.html). Trainers in Ray are required to perform distributed training. All we need to do is define a train_loop_per_worker function to perform the training steps that the different workers will use. To scale the training, we need to specify the Scaling Config. In addition, we must specify the Checkpoint config and Run Config that take care of the experiment's name, where the checkpoints will be stored, and any callbacks we are using. In this case, we will use mlflow to manage our experiments.\n",
    "6. **tune_engine.py**: This module is responsible for performing the tuning. It utilizes the Ray's TorchTuner. Details can be gathered from [HERE](https://docs.ray.io/en/latest/tune/key-concepts.html). In addition to the TorchTrainer, Scaling Config, Run Config, and Checkpoint Config, we define the search space, search algorithms, and schedulars. The tuning is performed in a distributed manner, and the experiments can be tracked using mlflow.\n",
    "7. **evaluate_engine.py**: This is the module for model evaluation as well as it can be adapted for batch inference.\n",
    "8. **serve_engine.py**: Lastly, this implementation also explores model serving using Gradio with Ray. \n",
    "\n",
    "Also, the **config.YAML** file has to be updated, especially for the directories used in the project.\n",
    "\n",
    "The code is in the form of Python scripts. To demonstrate how it works, in the notebook, these scripts are called using CLI commands. If you have any questions or comments, please do reach out!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c6814-7320-4030-b626-e4b307a7e735",
   "metadata": {},
   "source": [
    "---\n",
    "**Handling Custom Datasets in Different Scenarios**\n",
    "\n",
    "When you need to run the model on a custom dataset, you'll likely encounter two common scenarios: working with local data and working with remote data. Let's explore how to handle each scenario effectively:\n",
    "\n",
    "**Scenario 1: Local Data**\n",
    "\n",
    "Organizing Your Custom Dataset\n",
    "\n",
    "If your custom dataset is stored locally, you can ensure a smooth integration by following these steps:\n",
    "\n",
    "1. **Folder Structure**: Create a folder named `data` within your project directory. Inside this `data` folder, include subfolders for your custom dataset, such as `train`, `val` (validation), and `test`. Organize your data files accordingly.\n",
    "\n",
    "2. **Data Loading**: Modify your data loading process in the `train_engine.py` module. Instead of importing the `build_datasets_download` method, import the `build_datasets_local` method. This adjustment allows you to read the custom dataset from the local directory structure.\n",
    "\n",
    "**Scenario 2: Remote Data**\n",
    "\n",
    "Accessing Data from a Remote Source\n",
    "\n",
    "If your custom dataset is hosted remotely or available via a web service, you can access it as follows:\n",
    "\n",
    "1. **Data Retrieval**: Utilize methods or libraries to fetch the data from the remote source. This could involve using APIs, downloading data from a web URL, or accessing cloud-based storage.\n",
    "\n",
    "2. **Data Loading**: For the access of the data during training and tuning, you need to ensure that the correct function is imported from the 'data_utils.py' module. In this case, 'build_datasets_download' method needs to be imported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb34248-2e82-4e40-b052-9799b53bad42",
   "metadata": {},
   "source": [
    "---\n",
    "**How to Use the `train_engine.py` Module**\n",
    "\n",
    "Once you've tailored the configuration file to match your project's specifications, you can proceed with training the model using the `train_engine` module. This module can be executed via a command-line interface (CLI) command, simplifying the training process. Here's how to use it:\n",
    "\n",
    "**CLI Command**\n",
    "\n",
    "To initiate the training process, execute the following CLI command:\n",
    "\n",
    "```bash\n",
    "python train_engine.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "776f3763-acae-4a38-93ae-985b0629beeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-03 09:41:03</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:43.76        </td></tr>\n",
       "<tr><td>Memory:      </td><td>61.0/187.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/32 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  val_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_6c48f_00000</td><td>TERMINATED</td><td>192.168.13.177:11541</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         38.1492</td><td style=\"text-align: right;\">  0.142499</td><td style=\"text-align: right;\"> 0.973684</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=11541)\u001b[0m Starting distributed worker processes: ['11605 (192.168.13.177)', '11606 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11605)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11605)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11605)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11606)\u001b[0m Epoch 0-train Loss: 0.6301 Acc: 0.6803\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11606)\u001b[0m Epoch 2-train Loss: 0.3065 Acc: 0.9508\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11606)\u001b[0m Epoch 4-train Loss: 0.1939 Acc: 0.9344\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11606)\u001b[0m Epoch 6-train Loss: 0.1488 Acc: 0.9590\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11606)\u001b[0m Epoch 8-train Loss: 0.1364 Acc: 0.9508\u001b[32m [repeated 8x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 09:41:03,210\tINFO tune.py:1148 -- Total run time: 43.81 seconds (43.76 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(\n",
      "  metrics={'val_loss': 0.14249894532718158, 'val_acc': 0.9736842105263158, 'should_checkpoint': True, 'done': True, 'trial_id': '6c48f_00000', 'experiment_tag': '0'},\n",
      "  path='/home/dhaval/Projects/NewRay/results/ray_results/tuning-resnet-1693748416/TorchTrainer_6c48f_00000_0_2023-09-03_09-40-19',\n",
      "  checkpoint=TorchCheckpoint(local_path=/home/dhaval/Projects/NewRay/results/ray_results/tuning-resnet-1693748416/TorchTrainer_6c48f_00000_0_2023-09-03_09-40-19/checkpoint_000009)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%run ../src/train_engine.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db463aaf-1c53-4c79-ac5f-087d3b6cc677",
   "metadata": {},
   "source": [
    "---\n",
    "**How to Use the `tune_engine.py` Module**\n",
    "\n",
    "After understanding how to execute the `train_engine`, let's explore the usage of the `tune_engine` module. This module allows you to perform multiple hyperparameter tuning runs based on the parameter space defined within `tune_engine.py`. In the future, consider configuring this parameter space in a separate configuration file to abstract away code details.\n",
    "\n",
    "**Module Invocation**\n",
    "\n",
    "The `tune_engine.py` module does not require any command-line arguments. Instead, it accesses the necessary parameters and configuration settings from a specified config file. The absence of command-line arguments simplifies usage and reduces the likelihood of errors.\n",
    "\n",
    "**Configuration File**\n",
    "\n",
    "To use the `tune_engine` effectively, make sure you have a configuration file in place. This file should contain the required arguments for your tuning runs.\n",
    "\n",
    "**Hyperparameter Tuning**\n",
    "\n",
    "The primary purpose of the `tune_engine` module is to perform hyperparameter tuning, where it systematically explores various combinations of hyperparameters to find the best configuration for your machine learning model. This process can help optimize your model's performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "129d65a3-2e92-4cfd-ad76-a2fb824d7378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-09-03 10:01:52</td></tr>\n",
       "<tr><td>Running for: </td><td>00:07:15.57        </td></tr>\n",
       "<tr><td>Memory:      </td><td>61.2/187.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=20<br>Bracket: Iter 4.000: -0.15989777837928973 | Iter 1.000: -0.5315220501813057<br>Logical resource usage: 3.0/32 CPUs, 2.0/2 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">    train_loop_config/ba\n",
       "tch_size</th><th style=\"text-align: right;\">  train_loop_config/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  val_acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_6b353_00000</td><td>TERMINATED</td><td>192.168.13.177:16786</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">           0.000395467</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        38.9987 </td><td style=\"text-align: right;\">  0.190796</td><td style=\"text-align: right;\"> 0.960526</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00001</td><td>TERMINATED</td><td>192.168.13.177:17337</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.000238078</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.49993</td><td style=\"text-align: right;\">  0.660946</td><td style=\"text-align: right;\"> 0.631579</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00002</td><td>TERMINATED</td><td>192.168.13.177:17537</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">           0.000265461</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.22163</td><td style=\"text-align: right;\">  0.662777</td><td style=\"text-align: right;\"> 0.671053</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00003</td><td>TERMINATED</td><td>192.168.13.177:17692</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">           0.000387216</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.5228 </td><td style=\"text-align: right;\">  0.621326</td><td style=\"text-align: right;\"> 0.723684</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00004</td><td>TERMINATED</td><td>192.168.13.177:17856</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.001116   </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        18.3405 </td><td style=\"text-align: right;\">  0.437896</td><td style=\"text-align: right;\"> 0.960526</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00005</td><td>TERMINATED</td><td>192.168.13.177:18052</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.000850337</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.41257</td><td style=\"text-align: right;\">  0.625424</td><td style=\"text-align: right;\"> 0.697368</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00006</td><td>TERMINATED</td><td>192.168.13.177:18231</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">           0.0103263  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        35.745  </td><td style=\"text-align: right;\">  0.144249</td><td style=\"text-align: right;\"> 0.947368</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00007</td><td>TERMINATED</td><td>192.168.13.177:18504</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.0769279  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.5601 </td><td style=\"text-align: right;\">  6.28846 </td><td style=\"text-align: right;\"> 0.460526</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00008</td><td>TERMINATED</td><td>192.168.13.177:18693</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.0529297  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.41065</td><td style=\"text-align: right;\">  3.95463 </td><td style=\"text-align: right;\"> 0.460526</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00009</td><td>TERMINATED</td><td>192.168.13.177:18859</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.000290985</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.64748</td><td style=\"text-align: right;\">  0.642906</td><td style=\"text-align: right;\"> 0.644737</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00010</td><td>TERMINATED</td><td>192.168.13.177:19055</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.0290123  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.69077</td><td style=\"text-align: right;\">  0.742374</td><td style=\"text-align: right;\"> 0.592105</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00011</td><td>TERMINATED</td><td>192.168.13.177:19230</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">           0.00170909 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        37.1489 </td><td style=\"text-align: right;\">  0.143751</td><td style=\"text-align: right;\"> 0.986842</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00012</td><td>TERMINATED</td><td>192.168.13.177:19521</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.014571   </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        37.0344 </td><td style=\"text-align: right;\">  0.268241</td><td style=\"text-align: right;\"> 0.947368</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00013</td><td>TERMINATED</td><td>192.168.13.177:19783</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.00178917 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.61   </td><td style=\"text-align: right;\">  0.623116</td><td style=\"text-align: right;\"> 0.736842</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00014</td><td>TERMINATED</td><td>192.168.13.177:19945</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">           0.00177187 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">        38.2403 </td><td style=\"text-align: right;\">  0.140273</td><td style=\"text-align: right;\"> 0.973684</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00015</td><td>TERMINATED</td><td>192.168.13.177:20202</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">           0.0508837  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.85883</td><td style=\"text-align: right;\">264.643   </td><td style=\"text-align: right;\"> 0.526316</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00016</td><td>TERMINATED</td><td>192.168.13.177:20425</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.000261499</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.34984</td><td style=\"text-align: right;\">  0.713227</td><td style=\"text-align: right;\"> 0.578947</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00017</td><td>TERMINATED</td><td>192.168.13.177:20659</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">           0.011988   </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        18.5624 </td><td style=\"text-align: right;\">  3.84232 </td><td style=\"text-align: right;\"> 0.644737</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00018</td><td>TERMINATED</td><td>192.168.13.177:20961</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">           0.000257514</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.4244 </td><td style=\"text-align: right;\">  0.748843</td><td style=\"text-align: right;\"> 0.473684</td></tr>\n",
       "<tr><td>TorchTrainer_6b353_00019</td><td>TERMINATED</td><td>192.168.13.177:21156</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">           0.00502594 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        19.488  </td><td style=\"text-align: right;\">  0.164604</td><td style=\"text-align: right;\"> 0.960526</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=16786)\u001b[0m Starting distributed worker processes: ['16868 (192.168.13.177)', '16869 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 0-train Loss: 0.6773 Acc: 0.6066\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 0-val Loss: 0.5774 Acc: 0.8026\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 1-train Loss: 0.5965 Acc: 0.6967\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 1-val Loss: 0.4606 Acc: 0.9211\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 2-train Loss: 0.4511 Acc: 0.8934\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 2-val Loss: 0.3749 Acc: 0.9342\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 3-train Loss: 0.3966 Acc: 0.9098\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 3-val Loss: 0.3245 Acc: 0.9211\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 4-train Loss: 0.3464 Acc: 0.9180\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 4-val Loss: 0.2703 Acc: 0.9474\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 5-train Loss: 0.2938 Acc: 0.9344\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 5-val Loss: 0.2443 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 6-train Loss: 0.2794 Acc: 0.9180\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 6-val Loss: 0.2237 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 7-train Loss: 0.2599 Acc: 0.9180\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 7-val Loss: 0.2084 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 8-train Loss: 0.2272 Acc: 0.9426\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 8-val Loss: 0.1980 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 9-train Loss: 0.2143 Acc: 0.9344\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=16868)\u001b[0m Epoch 9-val Loss: 0.1908 Acc: 0.9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=17337)\u001b[0m Starting distributed worker processes: ['17413 (192.168.13.177)', '17414 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17413)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17413)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17413)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17413)\u001b[0m Epoch 0-train Loss: 0.6903 Acc: 0.5328\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17413)\u001b[0m Epoch 0-val Loss: 0.6609 Acc: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=17537)\u001b[0m Starting distributed worker processes: ['17575 (192.168.13.177)', '17576 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17575)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17575)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17575)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17575)\u001b[0m Epoch 0-train Loss: 0.6539 Acc: 0.6475\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17575)\u001b[0m Epoch 0-val Loss: 0.6628 Acc: 0.6711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=17692)\u001b[0m Starting distributed worker processes: ['17733 (192.168.13.177)', '17734 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17733)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17733)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17733)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17733)\u001b[0m Epoch 0-train Loss: 0.6883 Acc: 0.5738\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17733)\u001b[0m Epoch 0-val Loss: 0.6213 Acc: 0.7237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=17856)\u001b[0m Starting distributed worker processes: ['17905 (192.168.13.177)', '17906 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 0-train Loss: 0.6819 Acc: 0.6230\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 0-val Loss: 0.5742 Acc: 0.8947\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 1-train Loss: 0.6474 Acc: 0.6721\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 1-val Loss: 0.5557 Acc: 0.8158\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 2-train Loss: 0.6157 Acc: 0.7377\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 2-val Loss: 0.5112 Acc: 0.8816\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 3-train Loss: 0.5570 Acc: 0.7623\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17905)\u001b[0m Epoch 3-val Loss: 0.4379 Acc: 0.9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=18052)\u001b[0m Starting distributed worker processes: ['18091 (192.168.13.177)', '18092 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18091)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18091)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18091)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18091)\u001b[0m Epoch 0-train Loss: 0.6842 Acc: 0.5574\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18091)\u001b[0m Epoch 0-val Loss: 0.6254 Acc: 0.6974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=18231)\u001b[0m Starting distributed worker processes: ['18281 (192.168.13.177)', '18282 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 0-train Loss: 0.5917 Acc: 0.6639\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 0-val Loss: 0.2760 Acc: 0.9211\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 1-train Loss: 0.2538 Acc: 0.8852\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 1-val Loss: 0.1451 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 2-train Loss: 0.1636 Acc: 0.9344\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 2-val Loss: 0.1516 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 3-train Loss: 0.0660 Acc: 0.9836\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 3-val Loss: 0.1756 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 4-train Loss: 0.0874 Acc: 0.9672\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 4-val Loss: 0.2181 Acc: 0.9342\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 5-train Loss: 0.0413 Acc: 0.9918\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 5-val Loss: 0.3416 Acc: 0.9211\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 6-train Loss: 0.0837 Acc: 0.9672\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 6-val Loss: 0.3784 Acc: 0.9342\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 7-train Loss: 0.0141 Acc: 1.0000\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 7-val Loss: 0.3183 Acc: 0.9211\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 8-train Loss: 0.0702 Acc: 0.9754\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 8-val Loss: 0.1767 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 9-train Loss: 0.1577 Acc: 0.9590\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18281)\u001b[0m Epoch 9-val Loss: 0.1442 Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=18504)\u001b[0m Starting distributed worker processes: ['18542 (192.168.13.177)', '18543 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18542)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18542)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18542)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18542)\u001b[0m Epoch 0-train Loss: 0.5698 Acc: 0.6393\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18542)\u001b[0m Epoch 0-val Loss: 6.2885 Acc: 0.4605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=18693)\u001b[0m Starting distributed worker processes: ['18740 (192.168.13.177)', '18741 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18740)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18740)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18740)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18740)\u001b[0m Epoch 0-train Loss: 0.5845 Acc: 0.5902\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18740)\u001b[0m Epoch 0-val Loss: 3.9546 Acc: 0.4605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=18859)\u001b[0m Starting distributed worker processes: ['18937 (192.168.13.177)', '18938 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18937)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18937)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18937)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18937)\u001b[0m Epoch 0-train Loss: 0.6755 Acc: 0.5902\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18937)\u001b[0m Epoch 0-val Loss: 0.6429 Acc: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=19055)\u001b[0m Starting distributed worker processes: ['19099 (192.168.13.177)', '19100 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19099)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19099)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19099)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19099)\u001b[0m Epoch 0-train Loss: 0.6969 Acc: 0.4590\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19099)\u001b[0m Epoch 0-val Loss: 0.7424 Acc: 0.5921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=19230)\u001b[0m Starting distributed worker processes: ['19283 (192.168.13.177)', '19288 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 0-train Loss: 0.6555 Acc: 0.6066\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 0-val Loss: 0.4576 Acc: 0.9079\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 1-train Loss: 0.4108 Acc: 0.8197\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 1-val Loss: 0.2451 Acc: 0.9474\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 2-train Loss: 0.2629 Acc: 0.9262\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 2-val Loss: 0.1940 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 3-train Loss: 0.2229 Acc: 0.9016\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 3-val Loss: 0.1691 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 4-train Loss: 0.1446 Acc: 0.9590\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 4-val Loss: 0.1637 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 5-train Loss: 0.1475 Acc: 0.9426\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 5-val Loss: 0.1513 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 6-train Loss: 0.1260 Acc: 0.9590\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 6-val Loss: 0.1500 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 7-train Loss: 0.0855 Acc: 0.9754\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 7-val Loss: 0.1479 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 8-train Loss: 0.0618 Acc: 0.9836\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 8-val Loss: 0.1476 Acc: 0.9868\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 9-train Loss: 0.0846 Acc: 0.9590\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19283)\u001b[0m Epoch 9-val Loss: 0.1438 Acc: 0.9868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=19521)\u001b[0m Starting distributed worker processes: ['19561 (192.168.13.177)', '19562 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 0-train Loss: 0.6264 Acc: 0.5820\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 0-val Loss: 0.4593 Acc: 0.8421\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 1-train Loss: 0.4188 Acc: 0.8689\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 1-val Loss: 0.1879 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 2-train Loss: 0.2318 Acc: 0.9508\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 2-val Loss: 0.1508 Acc: 0.9474\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 3-train Loss: 0.1639 Acc: 0.9508\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 3-val Loss: 0.1368 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 4-train Loss: 0.1079 Acc: 0.9344\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 4-val Loss: 0.1418 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 5-train Loss: 0.0926 Acc: 0.9754\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 5-val Loss: 0.1734 Acc: 0.9474\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 6-train Loss: 0.0992 Acc: 0.9508\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 6-val Loss: 0.1806 Acc: 0.9474\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 7-train Loss: 0.0476 Acc: 0.9754\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 7-val Loss: 0.1882 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 8-train Loss: 0.0751 Acc: 0.9836\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 8-val Loss: 0.2327 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 9-train Loss: 0.0449 Acc: 0.9836\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19561)\u001b[0m Epoch 9-val Loss: 0.2682 Acc: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=19783)\u001b[0m Starting distributed worker processes: ['19828 (192.168.13.177)', '19829 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19828)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19828)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19828)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19828)\u001b[0m Epoch 0-train Loss: 0.7147 Acc: 0.5820\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19828)\u001b[0m Epoch 0-val Loss: 0.6231 Acc: 0.7368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=19945)\u001b[0m Starting distributed worker processes: ['19990 (192.168.13.177)', '19991 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 0-train Loss: 0.6301 Acc: 0.6721\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 0-val Loss: 0.4387 Acc: 0.9474\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 1-train Loss: 0.3865 Acc: 0.8607\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 1-val Loss: 0.2137 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 2-train Loss: 0.2682 Acc: 0.9098\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 2-val Loss: 0.1611 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 3-train Loss: 0.1717 Acc: 0.9426\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 3-val Loss: 0.1458 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 4-train Loss: 0.1394 Acc: 0.9508\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 4-val Loss: 0.1417 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 5-train Loss: 0.1238 Acc: 0.9426\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 5-val Loss: 0.1318 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 6-train Loss: 0.0906 Acc: 0.9836\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 6-val Loss: 0.1303 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 7-train Loss: 0.0834 Acc: 0.9754\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 7-val Loss: 0.1411 Acc: 0.9737\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 8-train Loss: 0.0934 Acc: 0.9754\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 8-val Loss: 0.1592 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 9-train Loss: 0.0759 Acc: 0.9590\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19990)\u001b[0m Epoch 9-val Loss: 0.1403 Acc: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=20202)\u001b[0m Starting distributed worker processes: ['20244 (192.168.13.177)', '20245 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20244)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20244)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20244)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20244)\u001b[0m Epoch 0-train Loss: 1.4841 Acc: 0.7459\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20244)\u001b[0m Epoch 0-val Loss: 264.6427 Acc: 0.5263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=20425)\u001b[0m Starting distributed worker processes: ['20468 (192.168.13.177)', '20469 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20468)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20468)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20468)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20468)\u001b[0m Epoch 0-train Loss: 0.7918 Acc: 0.4918\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20468)\u001b[0m Epoch 0-val Loss: 0.7132 Acc: 0.5789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=20659)\u001b[0m Starting distributed worker processes: ['20767 (192.168.13.177)', '20768 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 0-train Loss: 0.5407 Acc: 0.7131\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 0-val Loss: 0.5511 Acc: 0.8553\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 1-train Loss: 0.4344 Acc: 0.8525\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 1-val Loss: 2.9890 Acc: 0.6842\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 2-train Loss: 0.5361 Acc: 0.8525\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 2-val Loss: 9.9129 Acc: 0.6316\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 3-train Loss: 1.3305 Acc: 0.6803\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20767)\u001b[0m Epoch 3-val Loss: 3.8423 Acc: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=20961)\u001b[0m Starting distributed worker processes: ['21003 (192.168.13.177)', '21004 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21003)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21003)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21003)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21003)\u001b[0m Epoch 0-train Loss: 0.7006 Acc: 0.5164\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21003)\u001b[0m Epoch 0-val Loss: 0.7488 Acc: 0.4737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TorchTrainer pid=21156)\u001b[0m Starting distributed worker processes: ['21197 (192.168.13.177)', '21198 (192.168.13.177)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 0-train Loss: 0.6474 Acc: 0.5656\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 0-val Loss: 0.4727 Acc: 0.7895\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 1-train Loss: 0.4349 Acc: 0.7951\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 1-val Loss: 0.1965 Acc: 0.9605\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 2-train Loss: 0.2349 Acc: 0.9016\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 2-val Loss: 0.1718 Acc: 0.9474\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 3-train Loss: 0.1468 Acc: 0.9590\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=21197)\u001b[0m Epoch 3-val Loss: 0.1646 Acc: 0.9605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-03 10:01:52,206\tINFO tune.py:1148 -- Total run time: 435.62 seconds (435.57 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(\n",
      "  metrics={'val_loss': 0.14027262243785357, 'val_acc': 0.9736842105263158, 'should_checkpoint': True, 'done': True, 'trial_id': '6b353_00014', 'experiment_tag': '14_batch_size=32,lr=0.0018'},\n",
      "  path='/home/dhaval/Projects/NewRay/results/ray_results/tuning-resnet-1693749273/TorchTrainer_6b353_00014_14_batch_size=32,lr=0.0018_2023-09-03_09-54-36',\n",
      "  checkpoint=TorchCheckpoint(local_path=/home/dhaval/Projects/NewRay/results/ray_results/tuning-resnet-1693749273/TorchTrainer_6b353_00014_14_batch_size=32,lr=0.0018_2023-09-03_09-54-36/checkpoint_000009)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%run ../src/tune_engine.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d6c12-e9fa-4669-a15e-75323f6b041c",
   "metadata": {},
   "source": [
    "**Evaluation Module**\n",
    "\n",
    "This module is designed for evaluating the best model on a test/validation dataset. Here, we use the validation dataset, which is stored locally by default. If you need to evaluate the model on the test dataset, you will need to update the `data_utils.py` module accordingly.\n",
    "\n",
    "**Usage**\n",
    "\n",
    "To use this module, provide the name of the experiment as an argument when invoking the relevant function. The module will access the best run from the specified experiment and return the respective checkpoints for evaluation. The model loaded with the best checkpoint will be used to make predictions on the dataset.\n",
    "\n",
    "Note: The results on the entire validation set and the one that you saw in the tuning runs will be different because we are checkpointing results only from one worker which is exposed to half of the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "280e0bdf-4b1c-4a23-8b69-fdd384e9a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-03 10:02:37,510\tWARNING torch_predictor.py:57 -- You have `use_gpu` as False but there are 2 GPUs detected on host where prediction will only use CPU. Please consider explicitly setting `TorchPredictor(use_gpu=True)` or `batch_predictor.predict(ds, num_gpus_per_worker=1)` to enable GPU prediction.\n",
      "Accuracy:  0.954248366013072\n"
     ]
    }
   ],
   "source": [
    "!python ../src/evaluate_engine.py --experiment-name \"tuning-resnet-1693749273\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e099261-0ba9-4bf4-896d-173186b1358f",
   "metadata": {},
   "source": [
    "**Serving Module**\n",
    "\n",
    "The serving module is built using Gradio. This CLI command can be used to run the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5f0c8-3028-4401-80a5-a25279305e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/serve_engine.py --experiment_name \"tuning-resnet-1693749273\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
